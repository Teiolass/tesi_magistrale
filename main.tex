\documentclass[]{marticle}
\usepackage{mstyle}

\graphicspath{ {./imgs/} }

\title{\textbf{\huge Predicting and Explaining on Electronic Health Records  with Transformer-based Models: a use case}}
\date{}

\newcommand{\ds}{\mathcal{D}}
\newcommand{\codes}{\mathcal{C}}
\newcommand{\codesin}{\mathcal{C}_\text{in}}
\newcommand{\tree }{\mathcal{T}}
\newcommand{\patients}{\mathcal{P}}
\newcommand{\visits}{\mathcal{V}}


\begin{document}
\maketitle
\newpage

\section{Introduction}

Rapid growth of information systems in the healthcare sector has made large amounts of relevant data
available in the last decades. One very important and rich form of this kind of data is the
Electronic Health Records (EHR), among which we can find sequences of visit descriptions taken at
irregular time distances. Visits can consist in informations on the medical conditions, symptoms,
diagnoses, medications and actions taken during the admission. 

With large amounts of data available, a significant opportunity for the advancement of the quality of
the services offered is presented through its analysis. One important task is the prediction of
future analysis given the patient history in the form of EHR. This is a nontrivial task given the
high dimensionality of the problem and its temporal dependencies. However several approaches in the
field of deep learning have been proposed to tackle this task, mainly based on recurrent
architectures \deb{citazioni} while an exploration transformer-based architectures has just started
\deb{altre citazioni}.

Good predictive models are not enough though. Models as critical as those in the healthcare sphere
have to work in tandem with professionals. To increase the trustworthiness of the models,
predictions should not only accurate but also understandable by humans. However, inner logic of
state-of the-art deep-learning models is still poorly understood. Work has been conducted in the
last years to work around this limit, an important example being doctorXAI \cite{panigutti-xai}, an
agnostic explainable model specifically built to leverage ontological information. Its core idea is
to approximate the local behaviour of a opaque model with an intrisically explainable predictor.

In this thesis we want to improve and fully understand the capabilities of doctorXAI. Our aim is to
improve the predictive accuracy of the underlying model and to provide more insight into the
explanations provided by doctorXAI, to improve its usability and effectiveness for physicians. Thus
we work in several directions.
\begin{enumerate}
\item \emph{On learning a black box model:} We substitute the underlying recurrent black-box model
with a transformer-based one, trained on an updated version of the dataset. This allows for better
performances in the predictions.

\item \emph{On generating explanations:}  We propose a new method for generating a synthetic
neighborhood that should be aware of the underlying data distribution. This would allow for a more
thorough exploration of the local behavior of the predictive model. The process would be done by
adding noise to real data and then extracting samples over a distribution for the possible
reconstruction given by a deep learning model.

\item \emph{On explaining:} We try to explore some parts of the transformer black-box model
through the lens of the local explanation. In particular, we will notice some correlation between the
attention score given by the self-attention heads in the transformer and the feature importance
given by the tree in the local explanation pipeline.
\end{enumerate}

\section{Related}

\subsection{Machine Learning for Medical Data}

Qui si parla di drAI

\subsection{LLM and Transformers}

The development of predictor models for categorical sequential data has historically been deeply
intertwined with text generation and machine-translation tasks. In the most common approaches,
the text is divided into smaller portions called tokens, which usually represent a word, a suffix, a
punctuation symbol, or an indivisible part of the language. Typical values for the number
of tokens in a single language are in the order of a few tens of thousands.

The text generation task is the problem of generating a text answer to a text prompt, which can be a
question or some kind of instruction. The machine translation task is the problem of generating the
translation to some given input text. These two tasks are very similar, and for clarity, we will
focus on the translation one  in the following discussion.

Handling sequence of data of arbitrary length has always been a challenge in the context of deep
learning, due to the fixed size of the input and output layers in Neural Networks. The first
effective solution has been the idea of recurrent architectures.

The task is split in two parts: in the first part the model gets in input the tokens of the prompt
to translate. In the second part the answer is generated in an autoregressive manner: starting from
a special \c{<start>} token, the model generates the next token. The new token is then concatenated to
the previous ones, and is given to the model, which generates a next token. This process continues
until a special \c{<end>} token is generated, or the generated sequence length exceeds a fixed limit.

\subsubsection{Recurrent Models and Attention}

A recurrent model is built in such a way that it can take in input a hidden memory vector $h$, and a
token $t$, and then outputs $h'$, an updated version of $h$. The new hidden vector is then fed to
the model together with the next token.

The process is started with a fixed initial vector $h_0$, and let's assume the prompt is made by the
token sequence $t_0, \dots, t_n$. Then given the recurrent model $f$, the first hidden vector
generated is $h_1 = f(h_0, t_0)$, and in general we define $h_(i+i) = f(h_i, t_i)$ for $i=1\dots n$.
This tackles the first part of the problem. 

A very simple form for the model $f$ is the following. Let $e_1, \dots, e_N$ be a sequence of fixed
$k$-dimensional vectors (these are called embeddings), where $N$ is the number of possible tokens
and $k$ is a fixed integer. Let $sigma$ be a non linear activation function, typical choices are the
Rectified Linear Unit (ReLU) or the sigmoid function. Let $W_h$ and $W_t$ be two $n \times n$
matrices a $b$ a $k$-dimensional vector. Then if $h$ is a $k$-dimensional hidden state, and $t$ a
token corresponding to the vector $e_j$, we set $f(h,t) = \sigma(W_h h + W_t e_j + b)$.

Other more complex architectures has been explored, but this simple presented one is the base for
all of them.

For the second part we start with a the \c{<start>} token which we will call $g_0$. From this we
generate a new hidden state $\hat{h}_1 = f(h_{n+1}, g_0)$. With a classical NN model $f_\text{gen}$ we can
use $\hat{h}_1$ to generate a probability distribution $f_\text{gen} (\hat{h}_1)$ over all the possible
tokens. We can then extract a new token from this distribution. Usual approaches consists in taking
the maximum probability token, a plain sampling over the distribution, or some hybrid methods in
which the sample is performed only on the most probable tokens. The result of this process is a new
token $g_1$. From this point onward we repeat the process, generating $\hat{h}_2 = f(\hat{h}_1, g_1)$
and from the new hidden state a new token, and so on.

The training of this kind of models is performed noticing that it defines a probability distribution
over all the possible answer sentences. In fact each new token is generated from a probability
distribution given the previous ones. This is effectively a decomposition of the probability of the
whole sentence. Thus the training is performed maximizing the likelihood of a dataset of human
generated pairs of input and outputs.

\begin{figure}[!ht] 
\begin{center}
\includegraphics{recurrent_simple.pdf}
\caption{\deb{Recurrent architecture, simple.}} 
% \label{fig:your image label} 
\end{center}
\end{figure}

\begin{figure}[!ht] 
\begin{center}
\includegraphics{recurrent_context.pdf}
\caption{\deb{Recurrent architecture, with context.}} 
% \label{fig:your image label} 
\end{center}
\end{figure}

\begin{figure}[!ht] 
\begin{center}
\includegraphics{context_with_attention.pdf}
\caption{\deb{This is how we compute the context with attention}} 
% \label{fig:your image label} 
\end{center}
\end{figure}

\begin{figure}[!ht] 
\begin{center}
\includegraphics{self_attention_architecture.pdf}
\caption{\deb{This is the architecture of a self attention head. This structure is repeated for every index $j$}} 
% \label{fig:your image label} 
\end{center}
\end{figure}

As in most of trainings of Neural Networks, the optimization is done through Stochastic Gradient
Descent and backpropagation. This reveals the biggest weakness of recurrent architectures. Assuming
the model has parsed $n$ tokens between inputs and outputs, which means that reasonable values for
$n$ could range from a few dozens to few hundreds or even more, the gradient for the first
application of $f$ has to go through all the $n$ layers that follows to arrive to the loss. During
all these steps the quality of the gradient is degraded, usually leading to the phenomenons of
exploding gradients or vanishing gradients. The idea is that the flow of the gradient is a dynamical
system which can be mostly described by a multiplication by a a matrix. If these matrices
consistently increase the magnitude of the gradient, the gradient will become very large, and thus a
step of SGD will move the model parameters in a region in which the function cannot be approximated
by the gradient in the starting point. This means that the update is meaningless. If instead the
matrices consistently decrease the magnitude of the gradient, the update of the SGD will be near
zero, and the model will fail to converge.

Several attempts to solve this problem have been tried by modifying the inner architecture of the
recurrent net, and the most notable examples of that are the Gate Recurrent Units (GRUs) and the
Long Short Term Memory models (LSTMs). These are however only a mitigation of the problem, and not a
complete solution.

One first possible modification, that does not solve the gradient problem but still improves the
performances of the model is using two different models for the two parts of the task. These two
models have the same structure of the previous recurrent ones, but they are used for two different
objectives. The first part of the problem is to encode the input tokens into a single hidden vector,
and thus the model is called encoder. The second part of the problem is to take the hidden vector
and generate autoregressively the output, in fact decoding the vector into tokens. This second model
is thus called decoder.

A very important step for the evolution of these architectures has been in noticing that the hidden
state during the decoder phase of the process has two distinct functions: remembering the input, and
remembering at which point of the output generation the model is at. This suggest that the two
functions could be carried out by two different vectors. This can be easily accomplished by having
the decoder model be a function of a decoder hidden state, a context vector, which is the final
output of the encoder, and the previous token. The result is that the decoder is unable to forget
the encoder output, because it is not modified during the recurrence.

The next step has been improving this mechanism. In the translation task, it's very common for a
word in the output to map directly to a single word in the input, and more generally small groups of
words in the outputs correspond to small groups of words in the input. On the other hand most of the
data produced by the encoder, the hidden states, is discarded, keeping only the last hidden state.
As a consequence an arbitrary large amount of data, the input tokens, has to be compressed in a
single hidden state, whose size is fixed by the model architecture. The following idea improves over
this direction.

Let's assume that the encoder has generated the hidden states $h_0, \dots, h_n$, and let $\hat{h}$ be
the current hidden state in the decoder. In the previous architecture we would need a fixed context
vector generated from the encoder. What we do now is to generate a context vector dependent on
$\hat{h}$. The first step is to have a measure of how important is a given hidden state $h_i$
according to $\hat{h}$. In the most simple version of the architecture, this measure is simply the
dot product $h_i^\top \hat{h}$, but other differentiable functions of the two vectors could be used.
Once the importances $x_0, ..., x_n$ are given, they are converted on a distribution probability
over the hidden vectors of the encoder. This is done through the SoftMax function:
$$ \alpha_i = \frac{\exp (x_i)}{\sum_j \exp (x_j)}. $$ 
The idea is that the probability distribution measures how relevant is each hidden state for the
current prediction, and thus how much attention should be given to each of them. The distribution is
in fact called attention. With this in mind, the context vector is simply the mean of the hiddden
states, weighted through the attention:
$$ c = \sum_i \alpha_i h_i. $$

This is very important because attention allows the gradient to ``take shortcuts'' to get to the first
layers without having to pass through all the following ones.

One important way to scale up in size and complexity all the previous models is to have several
layer the recurrent architectures. These leads to have a first layer similar to the ones described
before: it takes as an input the previous hidden state, the embedding of a token, and in some cases
a context vector. Each of the subsequent layers takes in input the previous hidden state for that
layer, the hidden state of the previous layer, and sometimes the context vector.

\subsubsection{Transformers}

The next iteration on these design led to the current State of the Art models like GPT-3.5 or Llama.
The idea is to get rid entirely of the recurrent part, focusing only on the attention. The heart of
these models is the transformer. It is a layer that takes as an input a set of $d$-dimensional
vectors $e_1,..., e_n$ and returns a set of vectors of the same size. By means of a multiplication
by some fixed matrices $W_k$, $W_q$ and $W_v$ of size $d\times d_h$ where $d_h$ is called hidden
size, the vectors $e_i$ are mapped to three sets of vectors: keys $K_i = W_k e_i$, queries $Q_i =
W_q e_i$ and values $V_i = W_v e_i$. The keys perform the same role of the hidden states $h_i$ when
choosing the importances in the previous discussion. The queries are the anologous of the vector
$\hat{h}$, while the values are the vectors that will be combined through a weighted average.

For each query vector, a measure of importance is computed as before as the dot product with each
key value. This result is often scaled by the square root of the number of dimensions $d_h$, which
while it does not improve the expressiveness of the model, it balance the fact that increasing $d_h$
is reflected in bigger dot products. These importances are then processed in a SoftMax function to
give the attention probability distribution:
$$ \alpha_i = \text{SoftMax}((Q_i^\top K_0, \dots, Q_i^\top K_n) / \sqrt{d_h}) $$
where 
$$ \text{SoftMax}(y_1,...,y_n)_i = \frac{\exp(y_i)}{\sum_j \exp (y_j)}. $$
The output is again obtained by means of a convex combination:
$$ O_i = \sum_j \alpha_{i,j} V_j $$

The previous relationships can be expressed in a more compact form seeing the vectors $K_i$, $Q_i$
and $V_i$ as column of the matrices $K$, $Q$ and $V$. Thus we obtain:
\begin{equation}\label{attn-ref}
O = \text{SoftMax} (Q K^\top/\sqrt{d_h})V
\end{equation}

Since the keys, queries and values are obtained from the same set of vectors $e_i$, the expression
\ref{attn-ref} is called self-attention head.

Usually more than a single self-attention head is present in a transformer layer. In an
$m$-headed transformer there are $m$ triples of matrices $W_k^{(i)}$, $W_q^{(i)}$ and $W_v^{(i)}$,
and each of them is used to compute the respective output $O^{(i)}$ for $i=1,..., m$. These outputs
are concatenated and brought back to the right dimension by means of a linear projection through a
matrix $W_o$ of size $m d_h\times d$:
$$ y = \text{Concat}(O^{(1)},..., O^{(m)}) W_o. $$

Multiple transformer layers can be stacked on top of each other. Since the only source of
non-linearity is the SoftMax function, which appears only in the coefficients of the values, vectors
usually go through a Feed Forward Network (FFN) between a transformer layer and the other. These
networks are usually very simple, often in the form
$$ \text{FFN}(y_i) = W_1 \sigma(W_2 y_1 + b) $$
where $\sigma$ is an activation function, usually a ReLU, and $W_1, W_2, b$ are parameters of
suitable size. This component is also called Multi-Layer Perceptron (MLP).

As in most deep learning models, skip connections and layer normalizations are also added.
\deb{Di più?}

In the recurrent models the order of the tokens was given to the model implicitely, because the
their embedding were fed in the right order. The same does not apply in transformer models: a
permutation of the inputs leads just to the same permutation on the outputs. In other words the
transformer cannot distinguish the order of the inputs by itself, and additional informations are
needed. While this can add to the complexity of the model, it also means Transformers are more
flexible by recurrent models.

The most common way to add temporal information is performed by adding to each input vector of the
transformer $x_i$ a fixed vector dependent by time only $p_i$. Thus while $x_i$ changes for each run
of the transformer, which will have different inputs, $p_i$ is fixed among all these runs. There are
different choices for $p_i$. One is to have it completely learnable by SGD. Another is to fix it,
usually to a vector in the following form:
$$ p = \begin{pmatrix}
% \sin\frac{\omega i}{2^0} \\
\sin\omega^{1/d} i \\
\cos\omega^{1/d} i \\
\sin\omega^{2/d} i \\
\cos\omega^{2/d} i \\
\vdots \\
\sin\omega^{h/2d} i \\
\cos\omega^{h/2d} i
\end{pmatrix}
$$

where usually $\omega$ is taken to be 10,000.

This kind of vectors has showed to perform well. A possible line of reasoning for its justification
is that vectors of sines and cosines can be translated in time with just linear transformations,
which are easily performed within the model.

We observe that the length of the path between each output vector and each input embedding is not
dependent by the distance between the corresponding token, but they interact directly in the
attention computation. This completely solves the biggest issue of the recurrent models.

Another big advantage of transformer based models respect to recurrent ones is that all the
computations can be carried out in parallel, instead of having to find the hidden state of the
previous iteration in order to be able to start the next one. This allows a better usage of GPUs and
therefore smaller training times.

Moreover, in a training phase, all the outputs can be computed in one single sweep. This should be
done carefully though, because the prediction of the $i$-th cannot be performed by having as input
the $i+1$-th token, or even the $i$-th token itself. This can be done by tweaking the attention
matrices, imposing that the attention for the $i$-th query and the $j$-th key are zero whenever $i
\leq j$. The easiest way to accomplish this is by setting the corresponding importances to minus
infinity, which in turn will translate to zero through the SoftMax function. This correspond to add
to the importance matrices some upper triangular matrices.

Attention masks can be useful for another purpose as well. Until now we considered a single pair
input/output going through the model at each step of the training process. However usually deep
learning models are trained using minibatches of data for each step. This can be accomplished by
running multiple ``single pass'' steps, one for each element of the minibatch, and accumulating the
gradient. It may be more perfomant to deal with the minibatch in a single pass. This can be done by
concatenating all the data for the minibatch in a new dimension of the matrices, and the model
doesn't change much to accommodate this modification. However in general the sequence length will not
be constant through the minibatch. Thus all the sequences are padded to the max length within the
minibatch, and the attention mask is tweaked to avoid that any output depends on a padding token.

All of this tweaks allows the model to grow in size while still being trainable. In the last year in
fact there have been proposed several of such models, varying in size between a few billions
parameters to hundred of billions of parameters. These are called Large Language Models, or LLMs.
Examples are the GPT family developed by OpenAI, and the Llama family, developed by Meta.

\subsubsection{The Llama-2 Models}

The Llama-2 are a family of LLMs sharing the same underlying architecture and available in different
sizes: 7B, 13B, 34B and 70B, where the number counts the number of trainable parameters of the
model. The architecture is the same transformer-based decoder described in the previous section,
with the following modifications.
\begin{enumerate}
\item The normalization layers, used to prevent internal covariate shifts, are substituted with Root
Mean Square layer normalizations (RMSNorm) \cite{paper-rmsnorm}. The usual normalization layers
performs affine transformations on the hidden states such that the activations in a layer have mean
zero and standard deviation one. RMSNorm simply scales those activation such that the root mean
square is zero. This has showed to perform almost as better as the original version in terms of
loss, but is faster to compute.

\item The positional encoding have been replaced by Rotary Position Embeddings. The idea is to add
positional information at each layer of the model, modifying the key and query vectors by a
suitable function dependent by time. Let this modification be represented by the functions 
$f_k(y_n, n)$ and $f_q(y_n, n)$, where $y_n$ is the $n$-th vector of the previous layer or the
token embedding  multiplied by the matrix $W_k$ or $W_q$. Let's assume that the hidden dimension
to be an even number $d$, which is reasonable since for performance reasons typical values of
dimensions are large powers of two or some of their multiples. Then a good choice could be
$$ f_{\{q, k}\}(y_n, n) = R_{\Theta, n} y_m $$
where $R_(Theta, n)$ is a matrix in the following form:
$$
    R_{\Theta, n} = \begin{pmatrix} 
      R(n \theta_1)& 0& \dots& 0 \\
      0& R(n \theta_2)& \dots& 0 \\
      \vdots& \vdots& \ddots& \vdots \\
      0&0& \dots& R(n \theta_{d / 2})
    \end{pmatrix},
$$
$$
    R(\theta) = \begin{pmatrix} 
        \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta
    \end{pmatrix},
$$
and $\Theta = \{ \theta_1, \dots, \theta_{d / 2} \}$ with $\theta_i = 10000^{-2(i-1) / d}$.

This choice has a nice property. Let $x_n$ and $x_m$ be the outputs of the previous layer. Then
the attention importance relative to the $n$-th query and $m$-th key using the Rotary Embedding
can be written as
\begin{align}\label{rotary-importance}
I_{n,m} &= (R_{\Theta, n}W_q x_n)^\top (R_{\Theta, m}W_k x_m) \\ 
        &=  x_n^\top W_q^\top R_{\Theta, m-n} W_k x_m. \nonumber
\end{align}
The matrix $R_{\Theta, m-n}$ is an orthogonal matrix, which helps the flowing of the gradient
during training. Moreover from equation \ref{rotary-importance} we can see that this is a kind of
relative encoding, which, in contrast with the absolute encoding, add informations only about the
distance of two tokens, and not their position in the whole sentence. For many tasks this is a
desirable property. One final remark is that the matrices $R_(Theta, n)$ are very sparse, and thus
the computation of the products can be performed in a $\Ocompl(d)$ complexity.

\item The MLP layers has been replaced with a Swish Gated Linear Unit (SwiGLU). The function Swish
is defined as
$$ \text{Swish}(x) = x \sigma(x) $$
where $\sigma$ is the sigmoid function. Given a hidden dimension of the model $d_h$ and a new
dimension $d_\text{MLP}$ for the MLP, we define the Gated Linear Unit as be parametrized by the
$W_\text{gate}$, $W_\text{up}$ and $W_\text{down}$, of size $d_h\times d_\text{MLP}$ for the first
two, and $d_\text{MLP} \times d_h$ for the last one. Let's denote the component-wise multiplication
of two vectors with the symbol $\odot$. Then the layer is defined as $$ \text{MLP}(x) =
W_\text{down} (\text{Swish}(W_\text{gate} x) \odot W_\text{up} x) $$ This approach has been proven
to have a better behaviour than a plain MLP layer.
\end{enumerate}

\subsection{Explainability }

- Carrellata di roba

\subsubsection{DoctorXAI} \label{sect-drxai}

\deb{Credo che abbia più senso se messa dopo le sezioni in cui descrivo il data-type e
l'ontologia...}

DoctorXAI \cite{panigutti-xai} is a tool composed by two distinct parts. A predictor, called
doctorAI \cite{paper-doctor-ai}, and a novel explanation pipeline. \deb{Un capitolo a parte per
doctorAI?} For the explanation, given a patient history $p$, a neighborhood $N$ is generated. Then an
intrisically explainable predictor is trained on $N$. The final explanation is extracted from this
latter predictor.

\newcommand{\realds}{N_\text{real}}

Let $\ds$ be a dataset of real patients for explanation purposes. Let $d$ be the patient-to-patient
ontology-aware distance described in section \ref{sect-ontology}. We then extract from $\ds$ the $k$
patients that are the closest to $p$ according to $d$. This will form the real neighborhood
$\realds$ of $p$. The natural number $k$ is a parameter of the algorithm.

The following step is to augment $\realds$ with synthetic patients. In order to not introduce too
much noise into the final dataset $N$, synthetic patients are generated only through deletion from
real patients. The deletion is performed in an ontology-aware manner, as follows: given a patient
$p'\in \realds$, each code of its visits has a fixed probability $\theta\in [0,1]$ to be deleted. In
addition let $D=\{c_1, c_2, \dots, c_r\}$ be the set of such codes chosen for deletion. Let $c'_i$
be the parent of $c_i$ according to the ontology tree, for each $i=1,2,\dots,r$, and let $C'$ be the
sets of the $c'_i$ codes. Then the generated synthetic patient $p'$ is equal to $p$ when all the
codes whose parents appear in $C'$ have been deleted. We notice that for construction all the codes
in $D$ are deleted. In other words if a code is deleted, all the children of its parent are deleted
too.

We then need to associate a next visit prediction to each element of the neighborhood. This is done
by just running the black-box predictor on the data. A note on how the output is extracted should be
done. In fact the actual output of the black box predictor is a real number between zero and one for
each code, which defines the probability of that code to appear in the next visit according to the
predictor. Since these kind of predictors are evaluated with reacall at $k_\text{rec}$ metrics, the
predicted visit contains the $j$-th code when it is one of the first $k_\text{rec}$ most probable
codes. \deb{Non ho mai parlato di questa metrica prima.}

Since we are interested in explaing a specific prediction of a visit $v$, which thus contains
exactly $k_\text{rec}$ codes, we restrict the target labels through the whole dataset $N$ to those
codes. Therefore the target output has $k_\text{rec}$ dimensions.

We then have a dataset consisting in couples of patient history and prediction. The goal is to fit a
decision tree classifier on them, but unfortunatly one more step is needed to tackle the fact that
trees can only work on tabular data while we are dealing with complex data shapes. The proposed
solution is to encode all the codes in a single vector. Let $p=(v_1, v_2, \dots, v_n)$ be a patient
to be encoded. Each visit can be encoded in a fixed-size vector through a one hot encoding. If
$\codes$ is the set of all possible codes, the vector will have $|\codes|$ dimensions. Let
$\hat{v_i}$ be the codification of the visit $v_i$. Then
\begin{equation*}
    (\hat{v_i})_j = \begin{cases}
        1 &\text{if the $j$-th code appears in $\hat{v_i}$}  \\
        0 &\text{otherwise}.
    \end{cases}
\end{equation*}

Let $\lambda\in [0,1]$ be a fixed discount factor, parameter of the algorithm. In
\cite{panigutti-xai} a value of $\lambda = 1/2$ is used. Then we can aggregate the vectors
$\hat{v_i}$ with an exponential decay governed by the $\lambda$ factor. We thus obtain the final
encoding
\begin{equation*}
    \hat{p} = \sum_{i=1}^r \lambda^{r-i+1} v_i.
\end{equation*}

A tree is then fitted on this data, the inputs being $|\codes|$-dimensional real vectors and the
outputs being $k_\text{rec}$-dimensional vectors of zeros and ones. The explanation consists in the
branches taken by the patient to be explained $p$ when classifing it according to the trained tree.

Explanation are evaluated with three metrics:
\begin{enumerate}
\item Fidelity to the black box, which describes the perfomances of the tree predictions using the
black-box as the ground-truth. This metric is evaluated on a held-out set taken from the
neighborhood $N$. Since this is a multi-label classification task \cite{paper-multi-label}, a $F_1$
score with micro averaging.

\item A hit score, which measures how often the tree and the black-box models agree on the patient
to be explained. This is computed as the Hamming-distance between the two outputs.

\item A measure of complexity of the explanation. This is the depth of the leaf in the decision tree
corresponding to the patient to be explained, and represents the length of the explanation. This is
important because longer rules may be difficult to interpret by humans \cite{paper-lipton}.
\end{enumerate}

We can observe that there is a tradeoff to be done between the first two metrics and the third. In
fact deeper trees can lead to better agreements between the black-box and the explainable
classifier, but will lead to longer explanations.

\deb{We also notice that there is no metric available to understand how well the synthetic
neighborhood represents the data distribution...}

\section{Problem Description}

\subsection{Data Type} \label{sect-datashape}

Let $\codes$ be a finite set of discrete elements called codes. A visit is defined as any subset
of $\codes$. A patient is defined as a finite ordered sequence of visits. A code is allowed to
appear more than once within different visits of the same patient. Let $\visits$ be the set of all
possible visits, and $cal("P")$ be the set of all possible patient.

A dataset $\ds$ consists of a sequence of patients from $\patients$.

We assume that there exists a probability distribution $mu$ on the set $\patients$, which measures
the plausibility of each patient in a Bayesian sense, and we assume that $\ds$ is a sequence of
independent patients $p_1, \dots, p_N$ sampled from that distribution.

We can factor $\mu$ over each visit in the following way: let $p=(v_1, \dots, v_k)$ be a patient. Then
we decomposition is given by
$$ \mu(p) = \mu_0(v_1) \mu_\text{next} (v_2 | (v_1)) \mu_\text{next} (v_3 |
    (v_1, v_2)) \cdots \mu_\text{next} (v_k | (v_1, \dots, v_{k-1})) $$
where $\mu_0$ is a probability distribution over the initial visits $\visits$, and $\mu_\text{next}$ is a
probability distribution over the next visit given the preceding ones. In other words $\mu_\text{next} (v
| p')$ is the probability of $v$ being the next observed visit after seeing all the visits in $p'$.
\deb{Probabilmente ci dovrebbe essere una 'visita terminale' per dire quando la sequnza finisce.}

A predictor model $h(\cdot, \theta)$ parameterized by a vector of parameters $\theta \in \R^d$, for some
number of dimensions $d$, is a function
$$ h(\cdot, \theta) : \patients \arr [0,1]^{|\codes|}. $$
The underlying idea is for $h$ to be a predictor of the next visit for a given patient. In fact we
can see $h(p, \theta)$ as a distribution on the visits $\visits$, where we assume that each code is
chosen independently from each other, and the $i$-th code has a probability of appearing given by
the $i$-th component of $h(p, \theta)$. For example the vector $x=(x_1, \dots, x_{|\codes|})$ assigns
to the visit $v=(c_{i_1}, \dots, c_{i_n})$ the probability $\prod_j x_{i_j}$. In the following
discussion we will identify the vector $x$ with its distribution on $\visits$.

The objective of a training process is to find an optimal parameter $\hat{\theta}$ for which the
function $h(\cdot, \hat{\theta})$ is the best approximation of the distribution $\mu_\text{next}$. Since the
latter is never known, we will use the empirical distribution found in the dataset $\ds$ as its proxy.

\subsection{Ontology} \label{sect-ontology}

In many fields, and in particular the medical one, which is of our interest, some categorical data
is not just made of indistinguishable labels, but rather by leaf-nodes of a tree. This can be
formally described as follows. Let $\codesin$ be a set of ``inner nodes'', and let $\codes' = \codes
\cup \codesin$. Let $\tree$ be a tree with nodes $\codes'$ and whose leaves are all and only the
codes $\codes$. This tree is called ontology and gives some information over the codes in $\codes$
that appaeres in the dataset. We will assume that codes that are closer with respect to the distance
on the graph $\tree$ to be more similar than codes that are less close.

As done in \cite{panigutti-xai} we proceed to describe a distance on the possible patients $\patients$. To
do that, we will use a distance over the codes $\codes$ and the visits $\visits$.

We start describing a code-to-code distance. It is the Wu-Palmer similarity score, which is one of
the most commonly used for the ICD ontology. Let $c_1$ and $c_2$ be two codes in $\codes$. Let $L$
be their lowest common ancestor on the tree $\tree$, and let $R$ be the root of $\tree$. Let $d(c_1',
c_2')$ be the distance between codes in $\codes'$ which measures the smallest number of steps needed
to reach $c_2'$ starting from $c_1'$ and moving along the edges of the graph $\tree$. the Wu-Palmer
similarity score is then defined as
$$ \text{WuP}(c_1, c_2) = \frac{2 d(L,R)}{d(c_1,L) + d(c_2, L) + 2d(L,R)} $$

We observe that $0 \leq \text{WuP}(c_1, c_2) \leq 1$ for each pair of codes, and the minimum value $0$ is
reached when $L$ is the root of the tree, while the maximum value $1$ is reached when $c_1 = c_2 =
L$.

We can then define a visit-to-visit distance. The approach of \cite{panigutti-xai} is to use an edit
distance weighted through the Wu-Palmer similarity. However would need to choose an order between
the codes of each visit, while the codes do not have a natural order. For this reason we will follow
a different approach. Let $V_A = \{c^A_1, ..., c^A_a\}$ and $V_B = \{c^B_1,..., c^B_b\}$ be two visits
composed by $a$ and $b$ codes respectively. For each code in $V_A$ we choose the best similar code
in $V_B$. We them sum all of the distances between the best pairs to get an asymmetric distance:
$$ d^\visits _\text{asym} (V_A, V_B) = \sum_{i=1}^a \min_{j=1...b} \text{WuP}(v^A_i, v^B_j). $$
We can symmetrize the above expression taking the maximum of the two permutations:
$$ d^\visits  (V_A, V_B) = \max(d^\visits _\text{asym} (V_A, V_B), d^\visits _\text{asym} (V_B, V_A)). $$

Assuming to have precomputed a table with all the distance pairs $\text{WuP}(c_1, c_2)$, the computation
of $d^\visits (V_A, V_B)$ has a complexity of $\Ocompl(n^2)$, where $n$ is an upper limit on the
size of the visits.

Finally we are ready to describe a patient-to-patient distance $d^\patients$. We do that through
the Dynamic Time Warp (DTW) Algorithm. It gives a measure of similarity between two time series that can
differ in speed. The idea is to find associations between two elements of each series, in our
context they are visits of two patients, subject to the following constraints:
- Every element of the first sequence must be associated to an element of the first one;
- The first and the last elements of each sequence must be associated between them, but they not to
  be their only association.
- The associations must be monotonical: Let the $i$-th element of the first sequence is associated
  to the $j$-th element of the second one, and a similar things happens for the $i'$-th element in
  the first sequence and the $j'$-th element of the second one. Then $i<i'$ implies $j\leq j'$.
We can give a cost to each way of associating two series: it is the sum of the visit-to-visit
distances between each associated pair. The DTW similarity is then defined as the minimum cost
between all the associations that respect the previous conditions.

This optimization problem can be solved with a dynamic programming approach. Here it follows the
algorithm in pseudocode:
\deb{Spudoratamente preso da Wikipedia}
\begin{verbatim}
int DTWDistance(s: array [1..n], t: array [1..m]) {
    DTW := array [0..n, 0..m]
    
    for i := 0 to n
        for j := 0 to m
            DTW[i, j] := infinity
    DTW[0, 0] := 0
    
    for i := 1 to n
        for j := 1 to m
            cost := d(s[i], t[j])
            DTW[i, j] := cost + minimum(DTW[i-1, j  ],    // insertion
                                        DTW[i  , j-1],    // deletion
                                        DTW[i-1, j-1])    // match
    
    return DTW[n, m]
}
\end{verbatim}
\deb{Posso spiegare più in dettaglio qui}

It should be noted that the measure we have defined is not a distance in the sense of the metric
spaces, in fact it cannot guarantee the triangular inequality. Moreover if the number of visit per
patient is bounded by $m$ and the number of codes per visit is bounded by $n$, then there will be
$m^2$ iterations within the DTW algorithm, each of those will require a computation of the
visit-to-visit distance, which costs $\Ocompl(n^2)$ iterations. Thus the total cost of the
algorithm is $\Ocompl(m^2n^2)$.

\subsection{Solution Proposal}

\section{Implementation of the Solution}

\subsection{Black-Box Model} \label{sect-kelso}

We build a new black-box model based on transformer architecture modyfing the Llama LLM \deb{cito il
mio capitolo o il paper?}. We observe that the shape of the data of a LLM is simpler than the one we
are dealing with. In fact text is defined in this context as a pure sequence of tokens, while our
medical data is a sequence of sets of labels, as described in section \ref{sect-datashape}.

We will take advantage of the flexibility of the positional embeddings to pass the structure of the
data to the model. Let $p=(v_1, v_2, \dots, v_n)$ be a patient, where $v_i=\{c^{(i)}_1, c^{(i)}_2,
\dots, c^{(i)}_{m_i}\}$ are visits for $i=1,2,\dots, n$. Let $p'$ be the flattened sequence of codes
in $p$, in which the order within each visit is arbitrarily chosen:
\begin{equation*}
    p' = (c^{(1)}_1, c^{(1)}_2, \dots, c^{(1)}_{m_1}, c^{(2)}_1, \dots, c^{(2)}_{m_2}, c^{(3)}_1,
    \dots, c^{(n)}_{m_n}).
\end{equation*}

Then let $q$ be the sequence of natural numbers that specify the positional encoding, where $q_j$ is
equal to $r$ if $p'_j$ is a code corresponding to the $r$-th visit in $p$. We can then feed to the
transformer model the vector $p'$ to look up for the corresponding embeddings, one for each code,
and the vector $q$ to set the correct positional embedding. Since the positional embeddings are the
only part of the transformer where positions matters, the choice of the order of the codes when
forming the vector $p'$ does not affect the final result.

We notice that this could not be performed in a recurrent architecture, since permutations in the
order of the codes necessarily lead to different results.

Together with the tweaked positional embeddings a modification to the decoder mask should be done.
In fact keys and queries associated to codes within the same visit should be always allowed to
interact with each other. The mask should still prohibit codes in the future visits to ``look
into'' codes of previous visits. Thus the mask $M$ must have a block-wise triangular form, as
follows:
\begin{equation*}
    M_{i,j} = \begin{cases}
        0       & \text{when $q_i\leq q_j$} \\
        -\infty & \text{otherwise.}
    \end{cases}
\end{equation*}

At this stage the model is able to correctly handle the input data and its structure. However
transformers do not change the shape of the data, but while the input of the model has a length
equal to the number of the codes of the patient, the output should have a length equal to the number
of the visits. Since in general in the final layer of the transformer there will be a vector for
each code in $p'$, we adopt an additional layer to aggregate all the vectors regarding codes of the
same visit into a single one. This is called a pooling layer. We present two different versions of
it.

The simplest one performs a simple component wise average over the vectors to be aggregated. This
keeps the same ``consideration'' for each code in the visit, and has the nice properties to be
differentiable (which is needed for SGD), invariant under the permutations of the codes, and
the resulting vector does not have bigger magnitude for longer visits, as a sum would lead to.

The second version is parametrized, and allows for different codes to contribute in different
amounts to the visit vector. Let $O$ be the output of the transformer, so it is a $m \times h$
matrix, where $m$ is the number of the codes in the visit $m=\sum_i m_i$, and $h$ is the hidden
dimension. We pass $O$ trough an additional transformer layer to obtain a sort of ``importance''
matrix $X$. The transformer layer is made up as usual by a multi-self-attention head and a MLP
layer. Since the objective of this layer is to pool together the codes in the same visit, the
positional embeddings are disabled, and the mask allows keys and queries vectors to interact only if
they are relative to codes within the same visit. More formally, the mask $M$ is defined as 
\begin{equation*}
    M_{i,j} = \begin{cases}
        0       & \text{when $q_i = q_j$} \\
        -\infty & \text{otherwise.}
    \end{cases}
\end{equation*}

We now have to act on $X$ such that we can define a probability vector over each visit. We do this
separately for each of the $h$ dimensions of the hidden vectors. As usual this conversion is done
with a SoftMax function. Let $Y$ the output of this operation and $j$ a selected column,
$j\in\{1,2,\dots h\}$. The matrix $Y$ will have the same shape of $X$. Let $i$ a selected row index,
$i\in \{1,2,\dots m\}$, such that the it corresponds to the $k$-th visit. Let $a$ and $b$ the index
of the first and last codes respectively in that visit. This means that 
\begin{equation*}
    a = \sum_{r < k} m_r + 1
\end{equation*}
and 
\begin{equation*}
    b = \sum_{r \leq k} m_r.
\end{equation*}

Then we define
\begin{equation*}
   Y_{[a:b], j} = \text{SoftMax}(X_{[a:b], j}),
\end{equation*}
where $Y_{[a:b], j}$ denotes the vector such that $(Y_{[a:b], j})_l = Y_{a+l-1, j}$ for ${{l=1,..., b-1}}$.

The resulting Matrix of the pooling is computed by summing all the vectors in $X\odot Y$
corresponding to each visit.

With both the versions of the pooling head, the output is a matrix of shape $n \times h$ where $n$
is the number of visits. A simple linear layer will be enough to bring the output at a shape equals
to $n \times |\codes|$ that represent the logits for the presence of each codes in the prediction.

Of course only the last vector will be used for inference, but the whole matrix is needed for the
training.

Another modification over the original Llama model is that we use the standard normalization layer
instead of RMSNorm. We make this choice because training times are not very long with the amount of
data we have.

This model is trained with an Adam algorithm with Decoupled Weight Decay Regularization
\cite{paper-adamw}. For each visit to be predicted the loss is computed as the code-by-code binary
cross-entropy. This is then summed over the codes and averaged over all the visits of the minibatch.

We performed a hyperparameter selection with a complete search running the candidate models for a
fixed amount of epochs, and choosing the best on the metrics obtained on a held-out validation set
of the data.

The final model is then trained until the metrics on the validation set stop getting better. The
model is then evaluated on another held-out test set.

\subsection{Generative Explanation Pipeline}

Our second proposal is about the generation of the synthetic neighborhood for the explanation
pipeline. We start as in section \ref{sect-drxai} with a real neighborhood $\realds$ of patients
selected to be as close as possible to the patient to be explained $p$. The notion of ``closeness''
is again formalized by the ontology-aware visit-to-visit distance of section \ref{sect-ontology}.

The ontology pipeline modifies the real patients only by deletion, to avoid introducing noise into
the neighborhood. That would mean having to run the black-box model on patients out of the original
distribution of the data. Our goal is to find a way to explore the neighborhood of $p$ by augmenting
the the real neighbors with new codes that are plausible. The idea is to have a model that codifies
the distribution of the data, and let the generative process be guided by that model.

\deb{Ispirazione da stable diffusion?}

We build a model that is able to reconstruct the original patients from noisy ones. Given a real
patient $p_r$, the noise is added by masking some randomly chosen codes present in their visit with
a special code \c{hole}. The input has thus the same shape as described in section \ref{sect-kelso},
but the codes vector is able to contain the new special code.

The model is then composed by the following stack of layers:
\begin{enumerate}
\item An embedding layer that associates each code to a $h$-dimensional vector. It can accept
$|\codes|+1$ distinct codes.

\item Several transformer layers. These are built in the same way as in section \ref{sect-kelso},
with all the care needed to encode the correct structure of the data through the positional
embeddings and the decoder masks.

\item An output layer which is a simple linear function that shifts the dimension of the data from
$n \times h$, which is the output of the transformer, to $n \times |\codes|$.

\item A final SoftMax function, which converts the logits of the output layer into vector of
probability over the codes.
\end{enumerate}

The procedure for generating a synthetic patient from a real one $p_r$ is done as follows:
\begin{enumerate}
\item The codes of $p_r$ are converted to their flattened form, and a vector of positional encodings
$q$ is built during the process.

\item Some codes chosen at random in the flattened patient are substituted with the \c{hole} code.

\item The noisy patient is fed to the generative model. For each code in input we have a
distribution of probability over the possible codes that can be placed in that position. We expect
this probability to be near a deterministic identity function when restricted to the unmasked codes,
and to reconstruct the original patients when restricted to the \c{hole} codes. \deb{???}

\item New codes are sampled from the distributions corresponding to the \c{hole} codes. The sampled
elements will substitute the \c{hole}s to obtain the final generated patient.
\end{enumerate}

\section{Experiments}

\subsection{Mimic IV}

Mimic IV \cite{mimic-iv-cit} is a dataset contaning medical informations resulted from the collaboration
of the Beth Israeli Deaconess Medical Center (BIDMC) and the Massachusetts Institute of Technology
(MIT). Data is gathered as part of the routine activities at BIDMC, and processed at MIT.

The dataset contains informations about patients accessing the services at the emergency department
or Intensive Care Units (ICUs) between 2008 and 2019. Patients that were below age 18 at ther first
admission time were excluded. People known to require extra protection were excluded as well.
Between the raw data sources and the final published dataset, a step of deintefication of the
patients has been performed, removing all personal informations and translating by a random time all
the events regarding every single patient.

The dataset is divided into three separate modules: \c{hosp}, \c{icu}, and \c{note}.

The \c{hosp} module stores information regarding patient transfers, billed events, medication
prescription, medication administration, laboratory values, microbiology measurements, and provider
orders. This is the main source of data of this project.

The \c{icu} module contains all the information collected by the MetaVision clinical information
system for the ICU units.

The \c{note} module contains textual data of discharges, which gathers an in-depth summary of the
patients history during their stays at the hospital, and a section about radiology data.

Our main interest is in the \c{hosp} module, which contains among other things a table of admissions
and a table of diagnoses. The admissions is a table that associates a patient id, an admission id,
several temporal coordinates for the admissions such as the admission, registration and discharge
times. There are also information about the patient like language, insurance, race and marital
status. We will use only the patient and admission id, and the relative ordering given by the
admission time. This table contains 431,231 rows, each corresponding to a unique admission. 

The diagnoses table is much larger, containing 4,756,326 rows, each with a unique diagnose. A
diagnose is composed by a patient id and admission id, which correspond to the ones given in the
admission table, a couple icd-version and icd-code, and a numerical priority ranging 1-39 which
ranks the importance of the codes within the same visit.

ICD, the International Classification of Diseases, which is a system to classify diagnostic
statements maintained by the World Health Organization (WHO). There are several versions of these
codes, and Mimic-iv uses ICD-9 and ICD-10. The version is flagged in a specific column of the
diagnoses table. A code may indicate signs, symptoms, abnormal findings, complaints, social
circumstances, and external causes of injury or disease. In \ref{table:icd_examples} are reported some examples
of ICD-10 codes. ICD-9 codes are very similar.

\begin{table}[h]
\begin{center}
\begin{tabular}{  c  l  }
    \hline
    Code & Description \\ 
    \hline
    A00.0 & Cholera due to Vibrio cholerae 01, biovar cholerae \\ 
    E66.0 & Obesity due to excess calories \\ 
    I22.0 & Subsequent myocardial infarction of anterior wall \\ 
    F32.0 & Mild depressive episode \\ 
    O80.0 & Spontaneous vertex delivery \\ 
    S81.0 & Open wound of knee \\ 
    W58   & Bitten or struck by crocodile or alligator \\ 
    Z56.3 & Stressful work schedule \\ 
    Z72.0 & Tobacco use (Excl. tobacco dependence) \\ 
    \hline
\end{tabular}
\caption{\deb{Table todo}}
\label{table:icd_examples}
\end{center}
\end{table}

In the diagnoses table of the Mimic-iv dataset there are 2,766,877 ICD-9 codes (58\% of the total)
and 1,989,449 codes (42\% of the total). Considering codes with different versions of ICD
categorization as different, the dataset contains 25,829 distinct codes, which means that each code
appears on average 184 times in the database. Of course the codes are not evenly distributed among
those present. \deb{Grafico della distribuzione}.


The admission table contains 180,733 distinct patients. However there are 101,198 patients (56\% of
the total) who are present in a single admission. We will filter those out because to make a
prediction that can be compared with a ground truth at least two visits are necessary. A percentage
of 92\% of patients has no more than five visits.

A preprocess phase is done by deleting all the patients who have a single visit, and then a
conversion from ICD-10 to ICD-9 is performed, such that all the models will work on a single
ontology. The data that follows will be referred to the subset of patients with at least two visits
in the dataset.

The conversion between the two versions of the codification is not straightforward, and some ICD-10
codes can be mapped to more than a ICD-9 code. Moreover, the conversion table we used \deb{quale?}
does not contain all the ICD-10 codes present in Mimic-iv. These are the 0.8\% of all the codes, and
are assigned to a special ``not found'' class. When multiple conversions were available we chose a
random one, as done in \cite{setor-paper}.

We convert the ICD codes to another system called CCS. \deb{qui non trovo il paper, e in effetti
tutti i paper che lo usano linkano solo al sito web...}

\subsection{Analysis}

\begin{figure}[!ht] 
\center
\includegraphics{kelso-train-loss.pdf}
\caption{Loss for training Kelso} 
% \label{fig:} 
\end{figure}

\begin{figure}[!ht] 
\center
\includegraphics{kelso-train-recall.pdf}
\caption{Recalls for training Kelso} 
% \label{fig:} 
\end{figure}

\section{Conclusions}


\bibliography{refs}
\bibliographystyle{plain} % We choose the "plain" reference style
\end{document}
