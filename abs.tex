\documentclass[]{marticle}
\usepackage{mstyle}

\title{\textbf{\huge Predicting and Explaining on Electronic Health Records  with Transformer-based Models: a use case}}
\date{}
\author{}


\begin{document}
\maketitle

\textbf{Candidato:} Alessio Marchetti

\textbf{Relatori:} Roberto Pellungrini, Fosca Giannotti
\vspace{0.4cm}

Rapid growth of information systems in the healthcare sector has made large amounts of relevant
available in the last decades. One very common form of this kind of data is the Electronic Health
Records (EHR), which consists of a sequence of visit descriptions taken at irregular time
distances. Visits can consist in information on the medical conditions, symptoms, diagnoses,
medications and actions taken during the admission. World Health Organization (WHO) has made
available a standard codification for these descriptions called International Classification of
Diseases (ICD), which led to uniform data collected among different institutions.

Examples of these kinds of data are the Mimic datasets. They are collections of patient
demographics, symptoms, diagnoses, procedures and medical events gathered during the stays at the
Beth Israeli Deaconess Medical Center. Data has been processed at the Massachusetts Institute of
Technology. The last instance of the Mimic dataset is Mimic-iv, which improves on its predecessor in
several ways, one of the most important ones being the number of patients present in the data.

Together with the data availability a need has grown for its analysis. One important task has been
the prediction of future analysis given the patient history in the form of EHR. This is a non
trivial task given the high dimensionality of the problem and its temporal dependency. However
several approaches in the field of deep learning have been proposed, mainly based on recurrent
architectures, while transformer-based architectures have just started to be explored.

Good predictors are not enough though. Models as critical as those in the healthcare sphere have to
work in tandem with professionals. To increase the trustworthiness of the tools, predictions should
not only accurate, but also understandable by humans. However, there are still no methods to explicit
the inner reasoning of deep learning models in human terms. Work have been conducted in the last
years to work around this limit, an important example being doctorXAI. 

DoctorXAI is composed by two parts: a black-box predictor, called doctorAI, and an explanation
pipeline. The predictor is a model based on recurrent architecture with GRU cells. The explanation
is done by training an intrinsically explainable model, a decision tree classifier, on a local
neighborhood of the data whose prediction has to be explained. The neighborhood generation is guided
by a distance on the data points which is aware of the ontology present in the ICD codes.

This thesis expands the previous work in three directions:
\begin{enumerate}

\item We substitute the underlying recurrent black-box model with a transformer-based one, trained
on more data, updating the dataset used from Mimic-iii to Mimic-iv . We will show that our model
gives predictions which are more accurate than those of doctorAI. We leverage on the flexibility of
the positional encodings within the transformer architecture to deal with complex data shapes, in
our case sequences of subsets of a label set.

\item We propose a new method for generating a synthetic neighborhood which should be aware of the
underlying data distribution. This would allow for a more thorough exploration of the local behaviour
of the black-box. The process is done by adding noise to real data, and then extracting
samples over a distribution for the possible reconstructions given by a deep-learning model.

\item We try to understand some parts of the transformer black-box model through the lens of the
local explanation. In particular we will notice some correlation between the attention score given
by the self-attention heads in the transformer and the feature importances given by the tree in the
local explanation pipeline. This gives interesting insights over the inner workings of a model very
similar in structure to state of the art LLMs such as GPT-3.5 or Llama but smaller and easier to
analyze.

\end{enumerate}

\end{document}

