
@inproceedings{
panigutti-xai,
    address={Barcelona Spain}, title={Doctor XAI: an ontology-based approach to black-box sequential data classification explanations}, ISBN={978-1-4503-6936-7}, url={https://dl.acm.org/doi/10.1145/3351095.3372855}, DOI={10.1145/3351095.3372855}, abstractNote={Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.}, booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency}, publisher={ACM}, author={Panigutti, Cecilia and Perotti, Alan and Pedreschi, Dino}, year={2020}, month=jan, pages={629–639}, language={en} }

@article{
mimic-iv-cit,
    title={MIMIC-IV, a freely accessible electronic health record dataset}, volume={10}, ISSN={2052-4463}, DOI={10.1038/s41597-022-01899-x}, abstractNote={Abstract Digital data collection during routine clinical practice is now ubiquitous within hospitals. The data contains valuable information on the care of patients and their response to treatments, offering exciting opportunities for research. Typically, data are stored within archival systems that are not intended to support research. These systems are often inaccessible to researchers and structured for optimal storage, rather than interpretability and analysis. Here we present MIMIC-IV, a publicly available database sourced from the electronic health record of the Beth Israel Deaconess Medical Center. Information available includes patient measurements, orders, diagnoses, procedures, treatments, and deidentified free-text clinical notes. MIMIC-IV is intended to support a wide array of research studies and educational material, helping to reduce barriers to conducting clinical research.}, number={1}, journal={Scientific Data}, author={Johnson, Alistair E. W. and Bulgarelli, Lucas and Shen, Lu and Gayles, Alvin and Shammout, Ayad and Horng, Steven and Pollard, Tom J. and Hao, Sicheng and Moody, Benjamin and Gow, Brian and Lehman, Li-wei H. and Celi, Leo A. and Mark, Roger G.}, year={2023}, month=jan, pages={1}, language={en} }

@article{
setor-paper,
    title={Sequential Diagnosis Prediction with Transformer and Ontological Representation}, url={http://arxiv.org/abs/2109.03069}, abstractNote={Sequential diagnosis prediction on the Electronic Health Record (EHR) has been proven crucial for predictive analytics in the medical domain. EHR data, sequential records of a patient’s interactions with healthcare systems, has numerous inherent characteristics of temporality, irregularity and data insufﬁciency. Some recent works train healthcare predictive models by making use of sequential information in EHR data, but they are vulnerable to irregular, temporal EHR data with the states of admission/discharge from hospital, and insufﬁcient data. To mitigate this, we propose an end-to-end robust transformer-based model called SETOR, which exploits neural ordinary differential equation to handle both irregular intervals between a patient’s visits with admitted timestamps and length of stay in each visit, to alleviate the limitation of insufﬁcient data by integrating medical ontology, and to capture the dependencies between the patient’s visits by employing multi-layer transformer blocks. Experiments conducted on two real-world healthcare datasets show that, our sequential diagnoses prediction model SETOR not only achieves better predictive results than previous stateof-the-art approaches, irrespective of sufﬁcient or insufﬁcient training data, but also derives more interpretable embeddings of medical codes. The experimental codes are available at the GitHub repository1.}, note={arXiv:2109.03069 [cs]}, number={arXiv:2109.03069}, publisher={arXiv}, author={Peng, Xueping and Long, Guodong and Shen, Tao and Wang, Sen and Jiang, Jing}, year={2021}, month=sep, language={en} }

@article{
paper-doctor-ai,
    title={Doctor AI: Predicting Clinical Events via Recurrent Neural Networks}, abstractNote={Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients and 2,128 physicians over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform diﬀerential diagnosis with up to 79% recall@30, signiﬁcantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.}, author={Choi, Edward and Bahadori, Mohammad Taha and Schuetz, Andy and Stewart, Walter F and Sun, Jimeng}, pages={18}, language={en} }

@article{
paper-multi-label,
  author={Zhang, Min-Ling and Zhou, Zhi-Hua}, journal={IEEE Transactions on Knowledge and Data Engineering}, title={A Review on Multi-Label Learning Algorithms}, year={2014}, volume={26}, number={8}, pages={1819-1837}, doi={10.1109/TKDE.2013.39}}

@article{
paper-lipton,
    added-at = {2018-11-21T00:00:00.000+0100}, author = {Lipton, Zachary C.}, biburl = {https://www.bibsonomy.org/bibtex/25e0657b768a4829a518ae94b1a1a789b/dblp}, ee = {https://doi.org/10.1145/3233231}, interhash = {570f1885f8d1e4a9f49bf69b5c031cca}, intrahash = {5e0657b768a4829a518ae94b1a1a789b}, journal = {Commun. ACM}, keywords = {dblp}, number = 10, pages = {36-43}, timestamp = {2018-11-22T11:37:41.000+0100}, title = {The mythos of model interpretability.}, url = {http://dblp.uni-trier.de/db/journals/cacm/cacm61.html#Lipton18}, volume = 61, year = 2018 }

@misc{
paper-adamw,
      title={Decoupled Weight Decay Regularization}, author={Ilya Loshchilov and Frank Hutter}, year={2019}, eprint={1711.05101}, archivePrefix={arXiv}, primaryClass={cs.LG} }

@article{
paper-shickel-ehr-survey,
    title={Deep EHR: a survey of recent advances in deep learning techniques for electronic health record (EHR) analysis}, author={Shickel, Benjamin and Tighe, Patrick James and Bihorac, Azra and Rashidi, Parisa}, journal={IEEE journal of biomedical and health informatics}, volume={22}, number={5}, pages={1589--1604}, year={2017}, publisher={IEEE} }

@article{
paper-rmsnorm,
    title={Root Mean Square Layer Normalization}, url={http://arxiv.org/abs/1910.07467}, note={arXiv:1910.07467 [cs, stat]}, number={arXiv:1910.07467}, publisher={arXiv}, author={Zhang, Biao and Sennrich, Rico}, year={2019}, month=oct, language={en} }

 @article{
 xai-survey,
     title={Benchmarking and Survey of Explanation Methods for Black Box Models}, url={http://arxiv.org/abs/2102.13076}, abstractNote={The widespread adoption of black-box models in Artiﬁcial Intelligence has enhanced the need for explanation methods to reveal how these obscure models reach speciﬁc decisions. Retrieving explanations is fundamental to unveil possible biases and to resolve practical or ethical issues. Nowadays, the literature is full of methods with diﬀerent explanations. We provide a categorization of explanation methods based on the type of explanation returned. We present the most recent and widely used explainers, and we show a visual comparison among explanations and a quantitative benchmarking.}, note={arXiv:2102.13076 [cs]}, number={arXiv:2102.13076}, publisher={arXiv}, author={Bodria, Francesco and Giannotti, Fosca and Guidotti, Riccardo and Naretto, Francesca and Pedreschi, Dino and Rinzivillo, Salvatore}, year={2021}, month=feb, language={en} }
